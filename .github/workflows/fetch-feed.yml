name: Fetch and Clean RSS Feed

on:
  schedule:
    - cron: '0 */6 * * *'
  workflow_dispatch:

jobs:
  fetch:
    runs-on: ubuntu-latest
    
    permissions:
      contents: write
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
      
      - name: Install dependencies
        run: |
          pip install lxml requests beautifulsoup4
      
      - name: Fetch and Clean RSS Feed
        run: |
          python3 << 'EOF'
          import requests
          from lxml import etree
          from bs4 import BeautifulSoup
          from datetime import datetime, timezone
          import re
          from urllib.parse import quote

          # Fetch the original feed
          print("Fetching original feed...")
          response = requests.get('https://www.cabletv.com/feed')
          print(f"Fetched {len(response.content)} bytes")
          
          # Parse the XML
          root = etree.fromstring(response.content)
          channel = root.find('channel')
          
          # Get all items and limit to 8
          items = channel.findall('item')[:8]
          print(f"Processing {len(items)} items")
          
          # Define namespace map
          nsmap = {
              'content': 'http://purl.org/rss/1.0/modules/content/',
              'wfw': 'http://wellformedweb.org/CommentAPI/',
              'dc': 'http://purl.org/dc/elements/1.1/',
              'atom': 'http://www.w3.org/2005/Atom',
              'sy': 'http://purl.org/rss/1.0/modules/syndication/',
              'slash': 'http://purl.org/rss/1.0/modules/slash/',
              'media': 'http://search.yahoo.com/mrss/',
              'snf': 'http://www.smartnews.be/snf'
          }
          
          # Register all namespaces
          for prefix, uri in nsmap.items():
              etree.register_namespace(prefix, uri)
          
          # Fallback thumbnail
          fallback_thumbnail = 'https://ctv-clearlink.github.io/RSS-Feed/CableTV.com%20RSS%20Logo%20Header.png'
          
          # Build XML
          xml_parts = []
          xml_parts.append('<?xml version="1.0" encoding="UTF-8"?>')
          xml_parts.append('<rss version="2.0"')
          xml_parts.append(' xmlns:content="http://purl.org/rss/1.0/modules/content/"')
          xml_parts.append(' xmlns:wfw="http://wellformedweb.org/CommentAPI/"')
          xml_parts.append(' xmlns:dc="http://purl.org/dc/elements/1.1/"')
          xml_parts.append(' xmlns:atom="http://www.w3.org/2005/Atom"')
          xml_parts.append(' xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"')
          xml_parts.append(' xmlns:slash="http://purl.org/rss/1.0/modules/slash/"')
          xml_parts.append(' xmlns:media="http://search.yahoo.com/mrss/"')
          xml_parts.append(' xmlns:snf="http://www.smartnews.be/snf">')
          xml_parts.append('<channel>')
          
          # Add channel metadata
          for elem in channel:
              if elem.tag == 'item':
                  break
              tag = elem.tag
              if 'logo' in tag.lower():
                  continue
              
              elem_str = etree.tostring(elem, encoding='unicode', method='xml')
              xml_parts.append(elem_str)
              
              if elem.tag == 'description':
                  logo_filename = 'CableTV.com RSS Logo Header.png'
                  logo_url = f'https://ctv-clearlink.github.io/RSS-Feed/{quote(logo_filename)}'
                  xml_parts.append(f'<snf:logo><url>{logo_url}</url></snf:logo>')
          
          # Process items
          cdata_count = 0
          thumbnail_count = 0
          
          def score_image(src, position):
              """Score an image based on likelihood of being featured image"""
              score = 0
              src_lower = src.lower()
              
              # HIGHEST PRIORITY: Images with "watch-" prefix (featured images)
              if 'watch-' in src_lower:
                  score += 1000
              
              # High priority: .webp files (modern featured images)
              if src_lower.endswith('.webp'):
                  score += 500
              
              # Heavily penalize logos and icons
              if any(x in src_lower for x in ['logo', 'icon', 'avatar', 'badge']):
                  score -= 10000
              
              # Penalize very small images
              size_match = re.search(r'(\d+)x(\d+)', src)
              if size_match:
                  width = int(size_match.group(1))
                  height = int(size_match.group(2))
                  
                  if width < 200 or height < 200:
                      score -= 5000
                  elif width >= 600:
                      score += width + height  # Favor larger images
              
              # Prefer images earlier in content
              score -= position * 10
              
              # Look for common featured image patterns
              if any(x in src_lower for x in ['featured', 'hero', 'banner', 'main']):
                  score += 300
              
              # Prefer team/sports related images
              if any(x in src_lower for x in ['reds', 'phillies', 'mercury', 'nfl', 'mlb', 'nba', 'nhl']):
                  score += 200
              
              return score
          
          def find_best_image(html_content):
              """Find the best featured image from HTML content"""
              soup = BeautifulSoup(html_content, 'html.parser')
              all_imgs = soup.find_all('img')
              
              if not all_imgs:
                  return None
              
              # Score all images
              scored_images = []
              for position, img in enumerate(all_imgs):
                  src = img.get('src') or img.get('data-src', '')
                  if src:
                      score = score_image(src, position)
                      scored_images.append((score, src))
              
              if not scored_images:
                  return None
              
              # Return highest scoring image
              scored_images.sort(reverse=True)
              return scored_images[0][1]
          
          for idx, item in enumerate(items):
              title_elem = item.find('title')
              title_text = title_elem.text if title_elem is not None else 'Unknown'
              print(f"\nItem {idx + 1}: {title_text[:50]}")
              
              xml_parts.append('<item>')
              
              original_content = None
              content_html = None
              
              # Find content:encoded
              for elem in item:
                  tag_name = etree.QName(elem.tag).localname if '}' in elem.tag else elem.tag
                  namespace = etree.QName(elem.tag).namespace if '}' in elem.tag else None
                  
                  if namespace == 'http://purl.org/rss/1.0/modules/content/' and tag_name == 'encoded':
                      original_content = elem.text or ''
              
              # Process elements
              for elem in item:
                  tag_name = etree.QName(elem.tag).localname if '}' in elem.tag else elem.tag
                  namespace = etree.QName(elem.tag).namespace if '}' in elem.tag else None
                  
                  # Skip existing thumbnails
                  if namespace == 'http://search.yahoo.com/mrss/' and tag_name == 'thumbnail':
                      continue
                  
                  # Handle content:encoded
                  if namespace == 'http://purl.org/rss/1.0/modules/content/' and tag_name == 'encoded':
                      if original_content:
                          soup = BeautifulSoup(original_content, 'html.parser')
                          links = soup.find_all('a')
                          if len(links) > 3:
                              for link in links[3:]:
                                  link.unwrap()
                          content_html = str(soup)
                          
                          xml_parts.append(f'<content:encoded><![CDATA[{content_html}]]></content:encoded>')
                          cdata_count += 1
                  else:
                      elem_str = etree.tostring(elem, encoding='unicode', method='xml')
                      xml_parts.append(elem_str)
              
              # Add required fields
              if item.find('dc:creator', nsmap) is None:
                  xml_parts.append('<dc:creator>CableTV.com</dc:creator>')
              
              if item.find('pubDate') is None:
                  pubdate = datetime.now(timezone.utc).strftime('%a, %d %b %Y %H:%M:%S +0000')
                  xml_parts.append(f'<pubDate>{pubdate}</pubDate>')
              
              if item.find('description') is None and content_html:
                  text = re.sub(r'<[^>]+>', '', content_html)
                  text = text.strip()[:200] + '...'
                  text = text.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
                  xml_parts.append(f'<description>{text}</description>')
              
              # Find and add BEST thumbnail
              img_url = None
              if original_content:
                  img_url = find_best_image(original_content)
                  
                  if img_url:
                      img_url = img_url.strip()
                      
                      if img_url.startswith('//'):
                          img_url = 'https:' + img_url
                      elif img_url.startswith('/'):
                          img_url = 'https://www.cabletv.com' + img_url
                      elif not img_url.startswith('http'):
                          img_url = 'https://www.cabletv.com/' + img_url
                      
                      img_url = img_url.replace('&', '&amp;')
                      print(f"  ✓ Selected: {img_url.split('/')[-1]}")
                  else:
                      print(f"  ! No image found, using fallback")
                      img_url = fallback_thumbnail
              else:
                  img_url = fallback_thumbnail
              
              xml_parts.append(f'<media:thumbnail url="{img_url}"/>')
              thumbnail_count += 1
              
              xml_parts.append('</item>')
          
          xml_parts.append('</channel>')
          xml_parts.append('</rss>')
          
          xml_string = ''.join(xml_parts)
          
          xml_string = xml_string.replace(
              'https://www.cabletv.com/feed',
              'https://ctv-clearlink.github.io/RSS-Feed/feed.xml'
          )
          
          with open('feed.xml', 'w', encoding='utf-8') as f:
              f.write(xml_string)
          
          print(f"\n✓ Feed created with {cdata_count} CDATA sections")
          print(f"✓ Added {thumbnail_count}/{len(items)} thumbnails")
          print(f"✓ Output size: {len(xml_string):,} bytes ({len(xml_string)/1024/1024:.2f} MB)")
          EOF
      
      - name: Check for changes
        id: changes
        run: |
          git diff --quiet feed.xml || echo "changed=true" >> $GITHUB_OUTPUT
      
      - name: Commit and push feed
        if: steps.changes.outputs.changed == 'true'
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add feed.xml
          git commit -m "Update RSS feed $(date '+%Y-%m-%d %H:%M:%S')"
          git pull --rebase origin main
          git push origin main
